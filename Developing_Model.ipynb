{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing  Deep learning Models use Pytorch.nn.Moudule for beginners\n",
    "Here you will see how to build your own model using pytorch. We will start from the simplest to most complicated deep learning. There is a list of models we will cover in this tutorial. We only cover the programming here, if you are not familiar with the fundemetal of any of these you can easily search through internets.\n",
    "\n",
    "1. Regrassion\n",
    "2. Logistic Regression\n",
    "3. Customized Artificial Neural Network\n",
    "4. Recurrent Neural Network (Simple RNN, and LSTM)\n",
    "5. Convolution Neural Netwrk\n",
    "\n",
    "If you are familiar with Pytorch, you definitly know nn. Moudule Class enables programmer to build their customized model easily. You might think Keras is easier to work, but you will be suprised about the user firendly interface Pytorch is providing. In additon, when you can customised your model, you can build model combined two deep neural network together. For instance, think about predicting stock using RNN, you might think other features like weekday would effect the stock price, so, you can build model to combine classifer resutl with RNN result and make prediction!! Do you think is it amazing, let start learning how to build the Pytorch model as easy as Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Regression\n",
    "Regression is the simplest Artificial neural networks which transfers input_size=1 to output_size=1. To define the pytorch model, first you define the class name.\n",
    "Here we call it <b>LinearReg</b>. Then we define the <b>__init__</b> function with the inputs. Then, then just use <b>super(linearRegression, self).__init__()</b>.\n",
    "\n",
    "<img src="Reg.png" align=\"center\"/> \n",
    "\n",
    "\n",
    "These coding are the same for building all models. Now, you can build all layer you want here using torch.nn. Here, the order of the layer does not matter, so, just add as much as layer you want. For the regression, We only need one Linear transformation. All the class you define, has two parts, first the <b>__init__</b> function which enables you to determine all the parameters and layers you want. Second, it is the <b>forward</b> function which makes relation amongs all layers you have defined. Finally, you built the linearRegression model.\n",
    "\n",
    "`Attention!! In this file we are building models, in the next tutorial we will learn how to build the training loop.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearReg(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearReg, self).__init__()\n",
    "        self.linear = nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearReg(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LinearReg(1,1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2035], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable  \n",
    "model(FloatTensor([4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Logistic Regression\n",
    "Logistic Regression is very similar to Regression. We just need to add one sigmoid function after linear transformation of the resutls. If you want just build your model and do not use `the built-in Crossentropy` function. The model would be like below. However, if you will use 'built-in Crosssentropy' function, you can use the simple Regression model instead of Logistic Regression. The reason is that, the built-in Crosssentropy function use the unactivated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticReg(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LogisticReg, self).__init__()\n",
    "        #layers\n",
    "        self.linear = nn.Linear(inputSize, outputSize)\n",
    "\n",
    "        \n",
    "        #activation functions\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticReg(\n",
       "  (linear): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticReg(3,4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4655, 0.4006, 0.6527, 0.3259],\n",
       "        [0.5602, 0.4533, 0.5481, 0.4358],\n",
       "        [0.5782, 0.4029, 0.5316, 0.4494],\n",
       "        [0.5937, 0.4574, 0.5036, 0.4898],\n",
       "        [0.4947, 0.4248, 0.6338, 0.3311]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample = torch.rand(5,3)\n",
    "model(Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Artificial Neural Network\n",
    "Neural Network(NN) is constructed from several layers. NN is one of the simplest application of deep learning. In deep learning model, we ask the model to discover the hidden features. In contrast, the machine learning algorithm the machine know the features. We can see one NN architecture below (image from google):\n",
    "\n",
    "<img src='https://miro.medium.com/max/3000/1*HWhBextdDSkxYvz0kEMTVg.png' />\n",
    "\n",
    "As it is clear, we have three layers with different activation function. So, as we explained it in the regression, we start to add as much as layer we want. The layers convert data from size= 784 (as input size)==> first hidden layer size=128 ==> second hidden layer size=64 ==> output size=10. In addition, you can add as much as layer you want in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(NN, self).__init__()\n",
    "        #Hidden layers\n",
    "        self.layer1 = nn.Linear(inputSize, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        \n",
    "        #output layer\n",
    "        self.layer3 = nn.Linear(64, outputSize)\n",
    "\n",
    "        \n",
    "        #activation functions\n",
    "        self.relu=nn.ReLU()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NN(784,10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0989, 0.0938, 0.1007, 0.0976, 0.1053, 0.1091, 0.1055, 0.0919, 0.1007,\n",
       "         0.0965],\n",
       "        [0.0952, 0.0967, 0.1002, 0.0987, 0.1064, 0.1074, 0.1056, 0.0925, 0.0992,\n",
       "         0.0980],\n",
       "        [0.0943, 0.0948, 0.1052, 0.0984, 0.1044, 0.1089, 0.1034, 0.0910, 0.1013,\n",
       "         0.0983],\n",
       "        [0.0997, 0.0949, 0.1029, 0.0951, 0.1079, 0.1069, 0.1045, 0.0917, 0.1031,\n",
       "         0.0934],\n",
       "        [0.0966, 0.0947, 0.1021, 0.0974, 0.1028, 0.1110, 0.1028, 0.0945, 0.1018,\n",
       "         0.0963]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample = torch.rand(5,784)\n",
    "model(Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
